Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:01<00:00,  2.75it/s]
--> Model meta-llama/Llama-3.2-11B-Vision-Instruct

--> meta-llama/Llama-3.2-11B-Vision-Instruct has 10670.220835 Million params

trainable params: 5,898,240 || all params: 10,676,119,075 || trainable%: 0.0552
bFloat16 enabled for mixed precision - using bfSixteen policy
--> applying fsdp activation checkpointing...
--> Training Set Length = 210
--> Validation Set Length = 90
length of dataset_train 210
custom_data_collator is used
--> Num of Training Set Batches loaded = 26
--> Num of Validation Set Batches loaded = 11
--> Num of Validation Set Batches loaded = 11
model_name : meta-llama/Llama-3.2-11B-Vision-Instruct
tokenizer_name : None
enable_fsdp : True
low_cpu_fsdp : False
run_validation : True
batch_size_training : 1
batching_strategy : padding
context_length : 4096
gradient_accumulation_steps : 1
gradient_clipping : False
gradient_clipping_threshold : 1.0
num_epochs : 3
max_train_step : 0
max_eval_step : 0
num_workers_dataloader : 1
lr : 1e-05
weight_decay : 0.0
gamma : 0.85
seed : 42
use_fp16 : False
mixed_precision : True
val_batch_size : 1
peft_method : lora
use_peft : True
from_peft_checkpoint :
output_dir : PATH/to/save/PEFT/model
freeze_layers : False
num_freeze_layers : 1
freeze_LLM_only : False
quantization : None
one_gpu : False
save_model : True
dist_checkpoint_root_folder : /home/ubuntu/save_model/
dist_checkpoint_folder : checkpoints
save_optimizer : False
use_fast_kernels : True
use_wandb : True
save_metrics : True
flop_counter : False
flop_counter_start : 3
use_profiler : False
profiler_dir : PATH/to/save/profiler/results
dataset : custom_dataset


mixed_precision : True
use_fp16 : False
sharding_strategy : ShardingStrategy.FULL_SHARD
hsdp : False
sharding_group_size : 0
replica_group_size : 0
checkpoint_type : StateDictType.SHARDED_STATE_DICT
fsdp_activation_checkpointing : True
fsdp_cpu_offload : False
pure_bf16 : False
optimizer : AdamW


task_type : CAUSAL_LM
peft_type : LORA
auto_mapping : None
base_model_name_or_path : meta-llama/Llama-3.2-11B-Vision-Instruct
revision : None
inference_mode : False
r : 8
target_modules : {'q_proj', 'v_proj'}
exclude_modules : None
lora_alpha : 32
lora_dropout : 0.05
fan_in_fan_out : False
bias : none
use_rslora : False
modules_to_save : None
init_lora_weights : True
layers_to_transform : None
layers_pattern : None
rank_pattern : {}
alpha_pattern : {}
megatron_config : None
megatron_core : megatron.core
loftq_config : {}
eva_config : None
use_dora : False
layer_replication : None
runtime_config : LoraRuntimeConfig(ephemeral_gpu_offload=False)
lora_bias : False
_custom_modules : None
dataset : custom_dataset
file : /home/ubuntu/llama-cookbook-main_20250116/getting-started/finetuning/datasets/pg16_WPAFB_dataset.py
train_split : train
test_split : test
data_path :
Starting epoch 0/3
train_config.max_train_step: 0
/usr/lib/python3/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 1:   0%|[34m                                                                                          [0m| 0/26 [00:00<?, ?it/s][0m/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Training Epoch: 1/3, step 25/26 completed (loss: 3.9413559436798096): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 26/26 [01:17<00:00,  2.96s/it][0m
Max CUDA memory allocated was 5 GB
Max CUDA memory reserved was 8 GB
Peak active CUDA memory was 5 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 3 GB
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 11/11 [00:23<00:00,  2.11s/it][0m
 eval_ppl=tensor(75.0186, device='cuda:0') eval_epoch_loss=tensor(4.3177, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in PATH/to/save/PEFT/model directory
best eval loss on epoch 1 is 4.3177361488342285
Epoch 1: train_perplexity=13.2055, train_epoch_loss=2.5806, epoch time 78.0842207099995s
Starting epoch 1/3
train_config.max_train_step: 0
/usr/lib/python3/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 2:   0%|[34m                                                                                          [0m| 0/26 [00:00<?, ?it/s][0m/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training Epoch: 2/3, step 25/26 completed (loss: 0.5973504781723022): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 26/26 [01:08<00:00,  2.63s/it][0m
Max CUDA memory allocated was 5 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 5 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 48 GB
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 11/11 [00:23<00:00,  2.14s/it][0m
 eval_ppl=tensor(40.9890, device='cuda:0') eval_epoch_loss=tensor(3.7133, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in PATH/to/save/PEFT/model directory
best eval loss on epoch 2 is 3.7133045196533203
Epoch 2: train_perplexity=3.8226, train_epoch_loss=1.3409, epoch time 70.99557693399947s
Starting epoch 2/3
train_config.max_train_step: 0
/usr/lib/python3/dist-packages/torch/cuda/memory.py:365: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
Training Epoch: 3:   0%|[34m                                                                                          [0m| 0/26 [00:00<?, ?it/s][0m/usr/lib/python3/dist-packages/torch/utils/checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None
  warnings.warn(
Training Epoch: 3/3, step 25/26 completed (loss: 0.10138626396656036): 100%|[34mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 26/26 [01:08<00:00,  2.64s/it][0m           
Max CUDA memory allocated was 5 GB
Max CUDA memory reserved was 9 GB
Peak active CUDA memory was 5 GB
CUDA Malloc retries : 0
CPU Total Peak Memory consumed during the train (max): 48 GB
evaluating Epoch: 100%|[32mâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m| 11/11 [00:23<00:00,  2.14s/it][0m
 eval_ppl=tensor(38.6891, device='cuda:0') eval_epoch_loss=tensor(3.6556, device='cuda:0')
we are about to save the PEFT modules
PEFT modules are saved in PATH/to/save/PEFT/model directory
best eval loss on epoch 3 is 3.655557870864868
Epoch 3: train_perplexity=1.6635, train_epoch_loss=0.5090, epoch time 71.1888391299999s
Key: avg_train_prep, Value: 6.230571031570435
Key: avg_train_loss, Value: 1.4768438736597698
Key: avg_eval_prep, Value: 51.56557973225912
Key: avg_eval_loss, Value: 3.8955328464508057
Key: avg_epoch_time, Value: 73.42287892466629
Key: avg_checkpoint_time, Value: 33.010672480667075
Key: metrics_filename, Value: PATH/to/save/PEFT/model/metrics_data_0-2025-01-16_19-27-32.json
